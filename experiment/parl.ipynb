{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export PARL_BACKEND=torch\n",
    "import sys, os\n",
    "os.environ['PARL_BACKEND'] = 'torch'\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import parl\n",
    "# import paddle as torch\n",
    "# import paddle.nn as nn\n",
    "# import paddle.nn.functional as F\n",
    "# import parl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一层抽象，Model，是个策略网络(Policy Network)或者一个值函数网络(Value Function Network)\n",
    "# 这里我们写的是一个策略网络，它的输入是一个4维的向量，输出是一个2维的向量，分别是：[向左的概率，向右的概率]\n",
    "class CartpoleModel(parl.Model):\n",
    "    def __init__(self, obs_dim=4, act_dim=2):\n",
    "        super(CartpoleModel, self).__init__()\n",
    "        hid1_size = act_dim * 10\n",
    "        self.fc1 = nn.Linear(obs_dim, hid1_size)\n",
    "        self.fc2 = nn.Linear(hid1_size, act_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x是一个4维的向量，分别是：[cart_position, cart_velocity, pole_angle, pole_velocity]\n",
    "        out = torch.tanh(self.fc1(x))\n",
    "        prob = F.softmax(self.fc2(out))\n",
    "        return prob \n",
    "model = CartpoleModel(act_dim=2)\n",
    "# 定义Model的训练算法 Algorithm ，PolicyGradient是一个基于策略的强化学习算法。\n",
    "algorithm = parl.algorithms.PolicyGradient(model, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三层抽象，Agent，是一个智能体，它除了包含上面的一个策略网络，一个优化器\n",
    "# 还增加了 一个学习算法，一个经验池，一个探索策略。 主要就是获得数据，用数据区训练上面的Algorithm。\n",
    "# Agent 不包含 Environment，但是与 Environment交互\n",
    "class CartpoleAgent(parl.Agent):\n",
    "    def __init__(self, algorithm):\n",
    "        # 存储外面定义的算法\n",
    "        super().__init__(algorithm)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        # 根据环境状态返回动作（action），一般用于训练时候采样action进行探索。\n",
    "        # 这个是训练用的。\n",
    "        obs = torch.Tensor(obs).to(torch.float32)\n",
    "        prob = self.alg.predict(obs)\n",
    "        prob = prob.detach().numpy()\n",
    "        # 按照概率去选择一个动作，以便采样。\n",
    "        act = np.random.choice(len(prob), 1, p=prob)[0]\n",
    "\n",
    "        return act\n",
    "\n",
    "    def predict(self, obs):\n",
    "        # 根据环境状态返回预测动作（action），一般只是套一层算法的预测结果。\n",
    "        # 这个是最后的部署用的。\n",
    "        obs = torch.Tensor(obs).to(torch.float32)\n",
    "        prob = self.alg.predict(obs)\n",
    "        act = int(prob.argmax())\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward):\n",
    "        # 给出一个loss，以便反向传播？\n",
    "        act = np.expand_dims(act, axis=-1)\n",
    "        reward = np.expand_dims(reward, axis=-1)\n",
    "        obs = torch.Tensor(obs).to(torch.float32)\n",
    "        act = torch.Tensor(act).to(torch.int32)\n",
    "        reward = torch.Tensor(reward).to(torch.float32)\n",
    "        loss = self.alg.learn(obs, act, reward)\n",
    "        return float(loss)\n",
    "agent = CartpoleAgent(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 0, Reward Sum 18.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 10, Reward Sum 18.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 20, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 30, Reward Sum 20.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_542422/2014261775.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(self.fc2(out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 40, Reward Sum 16.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 50, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 60, Reward Sum 16.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 70, Reward Sum 28.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 80, Reward Sum 13.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 90, Reward Sum 17.0.\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:55]\u001b[0m Test reward: 1879.0\n",
      "\u001b[32m[04-11 21:18:40 MainThread @4191327931.py:44]\u001b[0m Episode 100, Reward Sum 21.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 110, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 120, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 130, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 140, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 150, Reward Sum 23.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 160, Reward Sum 20.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 170, Reward Sum 17.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 180, Reward Sum 17.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 190, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:55]\u001b[0m Test reward: 1854.0\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 200, Reward Sum 8.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 210, Reward Sum 38.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 220, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 230, Reward Sum 14.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 240, Reward Sum 19.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 250, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 260, Reward Sum 9.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 270, Reward Sum 17.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 280, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:41 MainThread @4191327931.py:44]\u001b[0m Episode 290, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:55]\u001b[0m Test reward: 1873.0\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 300, Reward Sum 18.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 310, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 320, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 330, Reward Sum 31.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 340, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 350, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 360, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 370, Reward Sum 9.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 380, Reward Sum 23.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 390, Reward Sum 17.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:55]\u001b[0m Test reward: 1864.0\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 400, Reward Sum 14.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 410, Reward Sum 22.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 420, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 430, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:42 MainThread @4191327931.py:44]\u001b[0m Episode 440, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 450, Reward Sum 19.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 460, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 470, Reward Sum 8.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 480, Reward Sum 19.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 490, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:55]\u001b[0m Test reward: 1846.0\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 500, Reward Sum 20.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 510, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 520, Reward Sum 14.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 530, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 540, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 550, Reward Sum 18.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 560, Reward Sum 18.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 570, Reward Sum 19.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 580, Reward Sum 14.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 590, Reward Sum 32.0.\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:55]\u001b[0m Test reward: 1864.0\n",
      "\u001b[32m[04-11 21:18:43 MainThread @4191327931.py:44]\u001b[0m Episode 600, Reward Sum 22.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 610, Reward Sum 22.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 620, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 630, Reward Sum 9.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 640, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 650, Reward Sum 25.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 660, Reward Sum 26.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 670, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 680, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 690, Reward Sum 13.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:55]\u001b[0m Test reward: 1894.0\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 700, Reward Sum 39.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 710, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 720, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 730, Reward Sum 27.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 740, Reward Sum 16.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 750, Reward Sum 21.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 760, Reward Sum 12.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 770, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:44 MainThread @4191327931.py:44]\u001b[0m Episode 780, Reward Sum 14.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 790, Reward Sum 26.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:55]\u001b[0m Test reward: 1872.0\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 800, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 810, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 820, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 830, Reward Sum 13.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 840, Reward Sum 21.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 850, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 860, Reward Sum 10.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 870, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 880, Reward Sum 14.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 890, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:55]\u001b[0m Test reward: 1895.0\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 900, Reward Sum 17.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 910, Reward Sum 9.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 920, Reward Sum 9.0.\n",
      "\u001b[32m[04-11 21:18:45 MainThread @4191327931.py:44]\u001b[0m Episode 930, Reward Sum 18.0.\n",
      "\u001b[32m[04-11 21:18:46 MainThread @4191327931.py:44]\u001b[0m Episode 940, Reward Sum 15.0.\n",
      "\u001b[32m[04-11 21:18:46 MainThread @4191327931.py:44]\u001b[0m Episode 950, Reward Sum 27.0.\n",
      "\u001b[32m[04-11 21:18:46 MainThread @4191327931.py:44]\u001b[0m Episode 960, Reward Sum 24.0.\n",
      "\u001b[32m[04-11 21:18:46 MainThread @4191327931.py:44]\u001b[0m Episode 970, Reward Sum 13.0.\n",
      "\u001b[32m[04-11 21:18:46 MainThread @4191327931.py:44]\u001b[0m Episode 980, Reward Sum 9.0.\n",
      "\u001b[32m[04-11 21:18:46 MainThread @4191327931.py:44]\u001b[0m Episode 990, Reward Sum 11.0.\n",
      "\u001b[32m[04-11 21:18:46 MainThread @4191327931.py:55]\u001b[0m Test reward: 1895.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "from parl.utils import logger\n",
    "# 训练代码\n",
    "def run_train_episode(env, agent):\n",
    "    obs_list, action_list, reward_list = [], [], []\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs_list.append(obs)\n",
    "        action = agent.sample(obs)\n",
    "        action_list.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward_list.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return obs_list, action_list, reward_list\n",
    "\n",
    "# evaluate 5 episodes\n",
    "def run_evaluate_episodes(env, agent, eval_episodes=200, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            action = agent.predict(obs)\n",
    "            obs, reward, isOver, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if isOver:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.sum(eval_reward)\n",
    "\n",
    "def calc_reward_to_go(reward_list, gamma=1.0):\n",
    "    for i in range(len(reward_list) - 2, -1, -1):\n",
    "        # G_i = r_i + γ·G_i+1\n",
    "        reward_list[i] += gamma * reward_list[i + 1]  # Gt\n",
    "    return np.array(reward_list)\n",
    "\n",
    "for i in range(1000):\n",
    "      obs_list, action_list, reward_list = run_train_episode(env, agent)\n",
    "      if i % 10 == 0:\n",
    "          logger.info(\"Episode {}, Reward Sum {}.\".format(i, sum(reward_list)))\n",
    "      batch_obs = np.array(obs_list)\n",
    "      batch_action = np.array(action_list)\n",
    "      batch_reward = calc_reward_to_go(reward_list)\n",
    "\n",
    "      agent.learn(batch_obs, batch_action, batch_reward)\n",
    "\n",
    "      if (i + 1) % 100 == 0:\n",
    "        #   _, _, reward_list = run_evaluate_episodes(env, agent)\n",
    "          reward = run_evaluate_episodes(env, agent)\n",
    "\n",
    "          logger.info('Test reward: {}'.format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
